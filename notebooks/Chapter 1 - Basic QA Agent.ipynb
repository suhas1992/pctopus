{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "767f1919-b2cc-4efb-9de4-0d14eaf237e8",
   "metadata": {},
   "source": [
    "# Building Our First AI Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e7e2f-09ef-4560-8b48-67273a9150eb",
   "metadata": {},
   "source": [
    "In this notebook, we'll build our first Question Answering (QA) agent using Large Language Models (LLMs). We're going to start with the simplest possible setup: a basic QA agent that takes a question in the form of a text and provides the answer in text using only the knowledge embedded in the LLM itself. \n",
    "\n",
    "This straightforward implementation serves as the foundation for our journey. As we progress, we'll evolve this basic agent into a sophisticated AI system capable of understanding your digital content, analyzing files of various formats, and helping you control your PC! "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b1d156b-7d94-4e20-b9be-e1dc1b469ea9",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a823fe-152f-4a7f-9758-2909cf60e137",
   "metadata": {},
   "source": [
    "### Setting Up Your OpenAI API Key\n",
    "\n",
    "To use the OpenAI API, you need to create a `.env` file in the same folder as your project and add your API key.\n",
    "\n",
    "#### Steps:\n",
    "1. Create a file named `.env` in your project folder.\n",
    "2. Add the following line to the file, replacing `sk-your-actual-key-here` with your actual OpenAI API key:\n",
    "   \n",
    "   ```env\n",
    "   OPENAI_API_KEY=sk-your-actual-key-here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aec3ef0-a3ca-4bdc-879c-dcfcdf3d52b7",
   "metadata": {},
   "source": [
    "### Don't Have an API Key?\n",
    "\n",
    "Follow these steps to generate one:\n",
    "\n",
    "1. Visit [platform.openai.com](https://platform.openai.com).\n",
    "2. **Sign up** or **Log in** to your account.\n",
    "3. Navigate to **Settings** → **API keys** → **Create new secret key**.\n",
    "4. Copy the generated API key and add it to your `.env` file.\n",
    "\n",
    "**Note**: Keep your API key secure and do not share it publicly!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acb97369-9dba-4245-8168-bcb7225491f6",
   "metadata": {},
   "source": [
    "### Let's set the environment up with the necessary dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9191ff88-ad00-4527-843a-735242a3e16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Get API key from .env file\n",
    "load_dotenv()\n",
    "client = openai.OpenAI(\n",
    "    api_key=os.getenv('OPENAI_API_KEY')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576236b0-6f8a-40b7-9282-4950c73f104e",
   "metadata": {},
   "source": [
    "## Your First AI Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76145909-2a4d-4020-9df7-e7956e96aad9",
   "metadata": {},
   "source": [
    "Let's start simple. We'll create a python function that takes your question and gets an answer from an OpenAI LLM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01422e48-275b-4ece-a762-80bed2555af6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: What is a Large Language Model in 2 sentences?\n",
      "A: A Large Language Model is a type of artificial intelligence algorithm that is trained on vast amounts of text data to generate human-like responses to text prompts. These models are capable of understanding and generating human language with high levels of accuracy and complexity.\n"
     ]
    }
   ],
   "source": [
    "def ask_ai(question: str) -> str:\n",
    "    \"\"\"Send a question to the LLM and get its response.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Try it out!\n",
    "question = \"What is a Large Language Model in 2 sentences?\"\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {ask_ai(question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9d7db8-98a0-4991-9f6a-a440ac4da29d",
   "metadata": {},
   "source": [
    "## Looking Under the Hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e195a28-6fbf-4563-97b9-db1caac330e6",
   "metadata": {},
   "source": [
    "Let's understand the code that we wrote in the `ask_ai` function. For the question-answer use case, we can use OpenAI's chat completions API (`client.chat.completions.create`). This is designed specifically for conversational AI where we send messages and get responses back from the LLM. Let's break down how it works:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a7d43a8-8559-4d8d-9fcd-5cfa28815f38",
   "metadata": {},
   "source": [
    "### Choosing the LLM \n",
    "\n",
    "If you see the first parameter of the chat completions API, we're specifying the **LLM model** that we want to use. \n",
    "\n",
    "OpenAI provides several LLM models (calle the GPT-series) to balance **capabilities**, **latency**, and **cost**. Below is a summary of 3 of the most commonly used models from OpenAI:\n",
    "\n",
    "#### 1. gpt-4o-mini\n",
    "- **Description**: A cost-effective and efficient model tailored for **fast, lightweight tasks**.\n",
    "- **Capabilities**: Accepts both **text** and **image** inputs, outputs **text**.\n",
    "- **Use Cases**: Ideal for basic queries, summaries, and low-complexity tasks.\n",
    "\n",
    "#### 2. gpt-4o\n",
    "- **Description**: A high-performing model optimized for **complex, multi-step tasks**.\n",
    "- **Capabilities**: Supports both **text** and **image** inputs, produces **text** output.\n",
    "- **Use Cases**: Best suited for detailed reasoning, problem-solving, and multi-modal tasks.\n",
    "- **Trade-offs**: Slower and more expensive compared to *gpt-4o-mini*.\n",
    "\n",
    "#### 3. gpt-3.5-turbo\n",
    "- **Description**: A fast and low-cost model with strong performance for most **text-only tasks**.\n",
    "- **Capabilities**: Accepts **text** input and provides **text** output.\n",
    "- **Use Cases**: Effective for applications like chatbots, coding assistance, and content generation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4ecb985-56a7-43ef-967f-14feab506145",
   "metadata": {},
   "source": [
    "### Messages and Roles\n",
    "\n",
    "The second parameter in the chat completions API is a `messages` parameter that represents a conversation history. Here, each message is defined by its role and content. Below is an example:\n",
    "\n",
    "```python\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"system\",    \n",
    "        \"content\": \"You are a helpful coding assistant\"  # Instructions for the AI's behavior\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"user\",      \n",
    "        \"content\": \"How do I write a for loop in Python?\"  # The user's message\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"assistant\", \n",
    "        \"content\": \"Here's how you write a for loop...\"  # The AI's response\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "469a41b7-6d9e-433e-b0e4-10e5003159ea",
   "metadata": {},
   "source": [
    "\n",
    "* `system`: Sets the basic instructions and behavior for the AI (e.g., \"You are a helpful coding assistant\"). This is typically used once at the start of a conversation.\n",
    "\n",
    "* `user`: Contains messages from the human user - their questions, requests, or statements. These form the main part of what the LLM responds to.\n",
    "\n",
    "* `assistant`: Contains the LLM's own previous responses in the conversation. These help maintain context and conversation flow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c347d-d06c-4de0-9cec-ee331d94a11a",
   "metadata": {},
   "source": [
    "### API Response Structure\n",
    "\n",
    "Let's examine how the chat completions API response is structured when we get a completion from the model. The heart of the response - the actual text generated by the LLM - is found in `response.choices[0].message.content`. You might wonder why we use `choices[0]` - this is because the API supports generating multiple alternative completions for the same prompt, though we typically only request one response. Think of it like asking multiple experts the same question and getting different perspectives!\n",
    "\n",
    "Key points to understand about the response structure:\n",
    "\n",
    "- The response is nested, with the content we want tucked inside the choices array\n",
    "- Each choice contains a message object with the actual response\n",
    "- The message includes both the content (the text) and the role (who's speaking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7380ed-a07a-4dde-853a-8128659b5d58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1️⃣ Basic Information - Metadata about our request\n",
      "ID: chatcmpl-AbdApw2C8rYWFcEjhhnfvlcVOrkB4\n",
      "Model: gpt-3.5-turbo-0125\n",
      "Created: 1733532083\n",
      "\n",
      "2️⃣ The Response Content - What the model actually said\n",
      "Content: The capital of France is Paris.\n",
      "Role: assistant\n",
      "Finish Reason: stop\n",
      "\n",
      "3️⃣ Usage Statistics - Token counts for billing and context length\n",
      "Prompt Tokens: 14\n",
      "Completion Tokens: 7\n",
      "Total Tokens: 21\n"
     ]
    }
   ],
   "source": [
    "# Here's how we make our request:\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",      # Which model we want to use\n",
    "    messages=[                   # The conversation history\n",
    "        {\"role\": \"user\",        # Who is speaking (user/assistant/system)\n",
    "         \"content\": question}   # What they're saying\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Now, let's see what we get back! We'll ask a simple question:\n",
    "question = \"What is the capital of France?\"\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[{\"role\": \"user\", \"content\": question}]\n",
    ")\n",
    "\n",
    "# Let's explore the response object:\n",
    "print(\"\\n1️⃣ Basic Information - Metadata about our request\")\n",
    "print(f\"ID: {response.id}\")                    # Unique identifier for this completion\n",
    "print(f\"Model: {response.model}\")              # Which model answered\n",
    "print(f\"Created: {response.created}\")          # Timestamp of the response\n",
    "\n",
    "print(\"\\n2️⃣ The Response Content - What the model actually said\")\n",
    "print(f\"Content: {response.choices[0].message.content}\")  # The model's answer\n",
    "print(f\"Role: {response.choices[0].message.role}\")       # Who's speaking (always 'assistant')\n",
    "print(f\"Finish Reason: {response.choices[0].finish_reason}\")  # Why the response ended\n",
    "\n",
    "print(\"\\n3️⃣ Usage Statistics - Token counts for billing and context length\")\n",
    "print(f\"Prompt Tokens: {response.usage.prompt_tokens}\")      # Tokens in our question\n",
    "print(f\"Completion Tokens: {response.usage.completion_tokens}\")  # Tokens in the answer\n",
    "print(f\"Total Tokens: {response.usage.total_tokens}\")        # Total tokens used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b0b4c-f54f-46ce-8d62-ceefd7b73da6",
   "metadata": {},
   "source": [
    "## What Happens Behind the Scenes\n",
    "\n",
    "When you send a prompt to the chat completions API, a fascinating process unfolds in milliseconds. Let's walk through this journey:\n",
    "\n",
    "1. **Breaking Down Text**  \n",
    "   Your text is split into smaller pieces called **tokens**. Think of tokens as the basic building blocks - they could be common words, parts of words, or even single characters. These are the building blocks of language that the model processes.\n",
    "\n",
    "2. **Translating to AI Language**  \n",
    "   These tokens then transform into **numerical vectors** (known as **embeddings**). Imagine this as translating human language into a mathematical form that AI understands - where words and concepts become coordinates in a vast **high-dimensional space**. In this space, similar concepts naturally cluster together.\n",
    "\n",
    "3. **Processing with Neural Networks**  \n",
    "   Now comes the real magic: the model processes these vectors through its **neural networks**. Using patterns it learned during training, it begins to understand your input's meaning, context, and implications.\n",
    "\n",
    "4. **Generating Output**  \n",
    "   The model then crafts its response, but interestingly, it does this **one token at a time**. Each new token is carefully chosen based on all the tokens that came before it, ensuring a coherent and contextual response.\n",
    "\n",
    "5. **Back to Human Language**  \n",
    "   These output tokens are converted back into **human-readable text**. What started as mathematical vectors return as natural language we can understand.\n",
    "\n",
    "6. **Adding Extra Insights**  \n",
    "   Finally, the API enriches the response with helpful metadata like **usage statistics**, giving us insight into how the model processed our request.\n",
    "\n",
    "This sophisticated embedding-based approach is what enables the model to grasp subtle connections between concepts and generate nuanced, context-aware responses. It's like having a brilliant conversationalist who can draw upon vast knowledge to engage in meaningful dialogue. 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba1e8c0-c18f-41b6-bab0-a93c6d0eae5b",
   "metadata": {},
   "source": [
    "## Enhancing Our AI Agent with System Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d525e6-f651-4b06-8d01-42efc564da08",
   "metadata": {},
   "source": [
    "So far, we've created a basic AI agent that can answer questions. However, we can make it more powerful and focused by adding system instructions. System instructions help define the AI's behavior, expertise, and constraints.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ea07ac3-7433-4fec-bdee-4759d8ce50dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: How do I read a CSV file in Python?\n",
      "A: ```python\n",
      "import csv\n",
      "\n",
      "# Open the CSV file in read mode\n",
      "with open('file.csv', mode='r') as file:\n",
      "    # Create a CSV reader object\n",
      "    csv_reader = csv.reader(file)\n",
      "    \n",
      "    # Iterate over each row in the CSV file\n",
      "    for row in csv_reader:\n",
      "        print(row)\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "def ask_ai_with_system_instruction(question: str, system_instruction: str) -> str:\n",
    "    \"\"\"Send a question to the LLM with a system instruction.\"\"\"\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_instruction},\n",
    "            {\"role\": \"user\", \"content\": question}\n",
    "        ]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "# Example: Creating a Python coding assistant\n",
    "system_instruction = \"\"\"You are a Python programming expert. \n",
    "Provide clear, concise code examples with comments. \n",
    "Only provide code and nothing else.\"\"\"\n",
    "\n",
    "question = \"How do I read a CSV file in Python?\"\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {ask_ai_with_system_instruction(question, system_instruction)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d2f4f2d-9427-4797-a068-9d695f406750",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q: Plot of the movie Titanic\n",
      "A: 🚢🌊💔🔥💏✨🎶🕰️🆘🧊💍👵🚁👩‍🦰👨‍🦰👵🛥️👱‍♂️🎨🎬\n"
     ]
    }
   ],
   "source": [
    "# Example: Creating a technical documentation writer\n",
    "system_instruction = \"\"\"Answer only using emojis\"\"\"\n",
    "\n",
    "question = \"Plot of the movie Titanic\"\n",
    "print(f\"Q: {question}\")\n",
    "print(f\"A: {ask_ai_with_system_instruction(question, system_instruction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1d9e3bc-364c-44ae-8f7f-d94a5cd12ff4",
   "metadata": {},
   "source": [
    "## Making it Interactive with Gradio"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ae96d7a-4706-4442-871b-5d21eb49084c",
   "metadata": {},
   "source": [
    "Let's create a simple web interface for our AI agent using Gradio. This will allow users to interact with the model through a user-friendly interface.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e81ec6f1-fe74-44b0-8cf8-d00918e0b576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "# Define our chat function for Gradio\n",
    "def chat_interface(message, system_instruction, history):\n",
    "    \"\"\"Handle chat interactions through Gradio.\"\"\"\n",
    "    response = ask_ai_with_system_instruction(message, system_instruction)\n",
    "    history.append({\"role\": \"user\", \"content\": message})\n",
    "    history.append({\"role\": \"assistant\", \"content\": response})\n",
    "    return \"\", history\n",
    "\n",
    "# Create the Gradio interface\n",
    "with gr.Blocks() as demo:\n",
    "    gr.Markdown(\"# Question-Answering AI Agent\")\n",
    "    \n",
    "    with gr.Row():\n",
    "        with gr.Column():\n",
    "            system = gr.Textbox(\n",
    "                label=\"System Instruction\",\n",
    "                placeholder=\"Set the AI's behavior...\",\n",
    "                value=\"You are a helpful assistant.\"\n",
    "            )\n",
    "    \n",
    "    chatbot = gr.Chatbot(type='messages')  # Specify type='messages' for the new format\n",
    "    msg = gr.Textbox(label=\"Message\")\n",
    "    clear = gr.Button(\"Clear\")\n",
    "\n",
    "    msg.submit(chat_interface, [msg, system, chatbot], [msg, chatbot])\n",
    "    clear.click(lambda: None, None, chatbot, queue=False)\n",
    "\n",
    "demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b74f83-43e4-4b8c-872a-c25814e7bfe4",
   "metadata": {},
   "source": [
    "## Limitations of the simple QA agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d32cac80-6495-433b-bec9-8e4678e6399f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing our AI's current capabilities:\n",
      "\n",
      "Q: What files are in my Downloads folder?\n",
      "A: I am sorry, but I am unable to access or view the content of your Downloads folder as I do not have the ability to access personal files on your device.\n",
      "\n",
      "Q: What's the weather in New York right now?\n",
      "A: I am sorry but I am unable to provide real-time weather updates. I recommend checking a reliable weather website or app for the most up-to-date information on the current weather in New York.\n",
      "\n",
      "Q: Can you read the PDF I just downloaded?\n",
      "A: As an AI text-based model, I'm unable to directly access or read files or browse the web. However, if you can provide me with the text content of the PDF document, I'd be happy to help you with any questions or information you may need.\n",
      "\n",
      "Q: Find the image where I'm in Machu Pichu\n",
      "A: I'm sorry, but as an AI, I do not have the ability to view or access images. I suggest searching for images of Machu Picchu and scanning through them to find the one with you in it.\n",
      "\n",
      "Q: What did we talk about yesterday?\n",
      "A: I'm sorry, but I do not have the ability to retain information from previous conversations. Can you please provide more context or details so I can better assist you?\n",
      "\n",
      "Q: Can you help me analyze data from my monthly_spending.csv file?\n",
      "A: Of course! Please provide me with the specific data you would like me to analyze from the monthly_spending.csv file.\n"
     ]
    }
   ],
   "source": [
    "limitations = [\n",
    "    \"What files are in my Downloads folder?\",\n",
    "    \"What's the weather in New York right now?\",\n",
    "    \"Can you read the PDF I just downloaded?\",\n",
    "    \"Find the image where I'm in Machu Pichu\",\n",
    "    \"What did we talk about yesterday?\",\n",
    "    \"Can you help me analyze data from my monthly_spending.csv file?\"\n",
    "]\n",
    "\n",
    "print(\"Testing our AI's current capabilities:\")\n",
    "for question in limitations:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {ask_ai(question)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f99b003-5060-40da-bdd6-a93889ec0d12",
   "metadata": {},
   "source": [
    "### Running this code, you'll notice our current QA agent has clear limitations:\n",
    "\n",
    "#### 1. It Cannot Access Your Files\n",
    "- Cannot read your documents, PDFs, or access your computer's folders\n",
    "- Only sees what you directly share in the conversation\n",
    "\n",
    "#### 2. It Lacks Multi-Modal Capabilities\n",
    "- Cannot process or understand images, audio or video input\n",
    "- No ability to work with different type of media files\n",
    "\n",
    "#### 3. It Cannot Control Your Computer\n",
    "- Unable to open applications\n",
    "- Cannot perform actions on your PC\n",
    "- No ability to interact with your system\n",
    "\n",
    "#### 4. It Has No Conversation Memory\n",
    "- Each chat starts completely fresh, like meeting someone for the first time\n",
    "- Previous conversations are not retained or accessible\n",
    "\n",
    "\n",
    "### These limitations exist because right now, our QA agent\n",
    "- Can't access external files or data\n",
    "- Can only handle text input and output\n",
    "- Has no ability to physically interact with your computer\n",
    "- Only processes the current query in isolation\n",
    "- Has no persistent memory between queries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60b4b443-081e-4cf6-b6cf-cec6b6a99060",
   "metadata": {},
   "source": [
    "## So What's Next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc861ab1-337f-4bc9-a560-6163087d0ab5",
   "metadata": {},
   "source": [
    "In the upcoming notebooks, we'll build increasingly capable agents:\n",
    "\n",
    "1. **Text-Enhanced Agent**\n",
    "\n",
    "- Read and understand text files\n",
    "- Answer questions using content from documents\n",
    "- Handle different text formats effectively\n",
    "\n",
    "2. **Media-Aware Agents**\n",
    "\n",
    "- Process and answer questions using images\n",
    "- Extract information from audio files\n",
    "- Understand and analyze video content\n",
    "- Combine all media types for comprehensive answers\n",
    "\n",
    "3. **File System Navigator**\n",
    "\n",
    "- Monitor and interact with your file system\n",
    "- Answer questions about your files and folders\n",
    "- Keep track of file changes automatically\n",
    "\n",
    "4. **System Control Agent**\n",
    "\n",
    "- Use tools to control your computer\n",
    "- Execute commands on your behalf\n",
    "- Automate computer tasks as needed\n",
    "\n",
    "Each notebook will build upon the previous one, gradually transforming our simple QA agent into a comprehensive PC assistant that can work with any type of file and help you control your computer! 🚀"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
