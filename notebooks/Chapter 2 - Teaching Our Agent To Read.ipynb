{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e2ffef81-7055-44af-9467-ebf41d654713",
   "metadata": {},
   "source": [
    "# Chapter 2: Teaching Our Agent to Read ðŸ“š\n",
    "\n",
    "In the previous chapter, we built a simple QA agent that relied solely on its pre-trained knowledge. While impressive, it had a significant limitation - it couldn't access or understand your personal documents and files. It's like having a brilliant friend who can tell you everything about world history but can't read the document right in front of them!\n",
    "\n",
    "\n",
    "In this chapter, we'll fix that by teaching our agent to read and understand document files. We'll also dive deep into fascinating concepts like context windows and explore the delicate balance between context size and speed. Let's get started! ðŸš€"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8272836-fc19-4ee4-a1c6-6e49f41cfbcf",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the `OpenAILLM` class that we coded in the previous chapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd8670e3-7709-4648-8d21-a5850c25d74f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pctopus.llm.openai_llm import OpenAILLM\n",
    "\n",
    "## Initialize the LLM\n",
    "llm = OpenAILLM()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887271ef-d833-44dd-8318-bdbfcfeb00ae",
   "metadata": {},
   "source": [
    "##  Teaching Our Agent to Use Context ðŸŽ“\n",
    "\n",
    "Let's start by creating a function that helps our agent use additional context when answering questions. By default, we'll have the agent use `gpt-3.5-turbo` model from OpenAI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db90cc86-0a78-4cf6-b126-7d04a59fcf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def ask_llm_with_context(context: str, question: str, system_instruction: Optional[str] = None, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Ask a question with specific context\"\"\"\n",
    "\n",
    "    question_with_context = f\"\"\"\n",
    "        Context: {context} \n",
    "        Question: {question}\n",
    "        \"\"\"\n",
    "    \n",
    "    response = llm.ask(\n",
    "        user_question=question_with_context,\n",
    "        system_message=system_instruction,\n",
    "        model=model\n",
    "    )\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416952fe-d442-4a9d-9fe2-4db4a3f319ef",
   "metadata": {},
   "source": [
    "#### Let's try it with a simple example where we give the LLM some context about me (Suhas), and ask it questions based on the context provided. In the system instruction, we ask the LLM to say \"I cannot find the answer in the provided context\" if the question asked can't be answered with the provided context. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3706f80c-0385-4e80-a653-2dd71c640a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing our context-aware QA agent:\n",
      "\n",
      "Q: Where was Suhas born?\n",
      "A: Suhas was born in Bangalore, India.\n",
      "\n",
      "Q: Where did he complete his masters from?\n",
      "A: Suhas completed his masters in Computational and Applied Mathematics at Stanford University.\n",
      "\n",
      "Q: Where does Suhas work now?\n",
      "A: Suhas works at Adobe as a Senior Machine Learning Engineer in the Search, Discovery, and Content AI team.\n",
      "\n",
      "Q: How many countries has Suhas visited?How many languages does Suhas know?\n",
      "A: I cannot find the answer in the provided context.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "Use the provided context to answer the question. \n",
    "If you cannot find the answer from the provided context, say \"I cannot find the answer in the provided context.\"\n",
    "\"\"\"\n",
    "\n",
    "context = \"\"\"\n",
    "Suhas was born in Bangalore, India. \n",
    "He did his undergraduate studies in Aerospace Engineering at IIT Madras in Chennai, India. \n",
    "He came to the United States for his masters in Computational and Applied Mathematics at Stanford Univeristy. \n",
    "He currently works at Adobe as a Senior Machine Learning Engineer in the Search, Discovery and Content AI team. \n",
    "\"\"\"\n",
    "\n",
    "questions = [\n",
    "    \"Where was Suhas born?\",\n",
    "    \"Where did he complete his masters from?\",\n",
    "    \"Where does Suhas work now?\",  \n",
    "    \"How many countries has Suhas visited?\" # Not in context\n",
    "    \"How many languages does Suhas know?\" # Not in context\n",
    "]\n",
    "\n",
    "\n",
    "print (\"Testing our context-aware QA agent:\")\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {ask_llm_with_context(context, question, system_instruction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55dd376-606d-4494-af85-748c820a3862",
   "metadata": {},
   "source": [
    "Great! Our agent can now:\n",
    "\n",
    "- âœ… Use provided context to answer questions\n",
    "- âœ… Admit when it can't find information in the context\n",
    "- âœ… Stick to the facts from the context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0430cf3c-a27a-4c29-9e92-9f687a5609a0",
   "metadata": {},
   "source": [
    "## Reading from Documents  ðŸ“„\n",
    "\n",
    "Now that our agent can use context, let's teach it to read text content from actual documents:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a79ebd8f-40c1-4193-a85b-6f2cc221b65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from pathlib import Path\n",
    "\n",
    "## This function is used to read from text files\n",
    "def read_txt_file(file_path: str) -> str:\n",
    "    \"\"\"Read content from a text file\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            return file.read()\n",
    "    except UnicodeDecodeError:\n",
    "        # Fallback encodings if UTF-8 fails\n",
    "        for encoding in ['latin-1', 'ascii', 'utf-16']:\n",
    "            try:\n",
    "                with open(file_path, 'r', encoding=encoding) as file:\n",
    "                    return file.read()\n",
    "            except UnicodeDecodeError:\n",
    "                continue\n",
    "        raise\n",
    "\n",
    "## This function is used to read the text content from PDF files\n",
    "def read_pdf_file(file_path: str) -> str:\n",
    "    \"\"\"Read content from a PDF file\"\"\"\n",
    "    try:\n",
    "        from PyPDF2 import PdfReader\n",
    "        reader = PdfReader(file_path)\n",
    "        return '\\n'.join(page.extract_text() for page in reader.pages)\n",
    "    except ImportError:\n",
    "        raise ImportError(\"PyPDF2 is required to read PDF files. Install it using: pip install PyPDF2\")\n",
    "\n",
    "## This function is used to read the text content from Word document (.doc, .docx)\n",
    "def read_word_file(file_path: str) -> str:\n",
    "    \"\"\"Read content from a Word document\"\"\"\n",
    "    try:\n",
    "        from docx import Document\n",
    "        doc = Document(file_path)\n",
    "        return '\\n'.join(paragraph.text for paragraph in doc.paragraphs)\n",
    "    except ImportError:\n",
    "        raise ImportError(\"python-docx is required to read Word files. Install it using: pip install python-docx\")\n",
    "\n",
    "## A common class that we can use to extract text content from text files, PDFs and Word document\n",
    "class DocumentReader:\n",
    "    def __init__(self):\n",
    "        # Register file formats and their corresponding reader functions\n",
    "        self.readers: Dict[str, Callable] = {\n",
    "            '.txt': read_txt_file,\n",
    "            '.pdf': read_pdf_file,\n",
    "            '.doc': read_word_file,\n",
    "            '.docx': read_word_file,\n",
    "        }\n",
    "\n",
    "    def supported_formats(self) -> list[str]:\n",
    "        \"\"\"Return list of supported file formats\"\"\"\n",
    "        return list(self.readers.keys())\n",
    "\n",
    "    def read_document(self, file_path: str) -> str:\n",
    "        \"\"\"\n",
    "        Read content from a document file based on its extension\n",
    "        \n",
    "        Args:\n",
    "            file_path (str): Path to the document file\n",
    "            \n",
    "        Returns:\n",
    "            str: Text content of the document\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If the file format is not supported\n",
    "            FileNotFoundError: If the file doesn't exist\n",
    "        \"\"\"\n",
    "        if not os.path.exists(file_path):\n",
    "            raise FileNotFoundError(f\"File not found: {file_path}\")\n",
    "\n",
    "        file_extension = Path(file_path).suffix.lower()\n",
    "        \n",
    "        reader = self.readers.get(file_extension)\n",
    "        if reader is None:\n",
    "            supported = ', '.join(self.supported_formats())\n",
    "            raise ValueError(f\"Unsupported file format: {file_extension}. Supported formats are: {supported}\")\n",
    "            \n",
    "        return reader(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43aa526a-5621-4fbb-ae48-168088e318b9",
   "metadata": {},
   "source": [
    "##### We'll feed the entire context from the document to the LLM agent using the `ask_llm_with_context` function that we wrote above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6efb70a7-1cc2-42bd-97e9-1fbcd49fd23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_reader = DocumentReader()\n",
    "\n",
    "def ask_llm_from_document(file_path: str, question: str, system_instruction: Optional[str] = None, model: str = \"gpt-3.5-turbo\") -> str:\n",
    "    \"\"\"Ask a question using text content from a document file as context.\"\"\"\n",
    "    document_text_content = doc_reader.read_document(file_path)\n",
    "    return ask_llm_with_context(document_text_content, question, system_instruction, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5597115-a3f6-4159-86a8-05e6452c3ca4",
   "metadata": {},
   "source": [
    "### Let's now test it with my LinkedIn profile, which is saved as a PDF\n",
    "\n",
    "This PDF is located at `documents/suhas_suresha_linkedin_profile.pdf`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6f71089e-832a-4e54-a5bf-132a91383a7b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Â  Â \n",
      "Contact\n",
      "www.linkedin.com/in/suhas-\n",
      "suresha  (LinkedIn)\n",
      "Top Skills\n",
      "Time-Series Prediction\n",
      "Generative Modeling\n",
      "MLOps\n",
      "Languages\n",
      "English  (Native or Bilingual)  \n",
      "Kannada  (Native or Bilingual)  \n",
      "Hindi  (Limited Working)\n",
      "Tamil  (Limited Working)\n",
      "Honors-Awards\n",
      "Schlumberger Innovation Fellow\n",
      "Publications\n",
      "Nonlinear dynamics and  intermittency \n",
      "in a turbulent reacting  wake with \n",
      "density ratio as bifurcation  parameter\n",
      "Capturing Uncertainty And  Preserving \n",
      "Geological Realism  with Attentive \n",
      "Neural Processes for  Subsurface \n",
      "Property Modeling\n",
      "Automated staging of knee  \n",
      "osteoarthritis severity using deep  \n",
      "neural networks\n",
      "Probabilistic Semantic Inpainting  with \n",
      "Pixel Constrained CNNs\n",
      "Patents\n",
      "System and method for identifying  \n",
      "subsurface structures\n",
      "Super resolution machine learning  \n",
      "model for seismic visualization  \n",
      "generation\n",
      "Rig state detection using video data\n",
      "Generalizable machine learning  \n",
      "algorithms for flash calculationsSuhas Suresha\n",
      "Machine Learning Engineer\n",
      "San Francisco Bay Area\n",
      "Summary\n",
      "I am a machine learning scientist with 7+ years of work experience.\n",
      "I have worked on several ML projects in the fields of computer\n",
      "vision, natural language processing, time-series prediction, and\n",
      "generative modeling. Additionally, I have experience in machine\n",
      "learning operations (MLOps), having built and deployed production-\n",
      "grade ML models and pipelines. I also have experience in cloud\n",
      "backend and iOS app development.\n",
      "Experience\n",
      "Adobe\n",
      "Senior Machine Learning Engineer  \n",
      "October 2024Â -Â PresentÂ (3 months)  \n",
      "San Francisco Bay Area\n",
      "Forward\n",
      "Engineer\n",
      "February 2023Â -Â June 2024Â (1 year 5 months)\n",
      "San Francisco Bay Area\n",
      "Worked on a mission to get the world's best healthcare to a billion people for\n",
      "free.\n",
      "QALY\n",
      "Co-Founder and Head of ML\n",
      "November 2020Â -Â December 2022Â (2 years 2 months)\n",
      "San Francisco Bay Area\n",
      "The QALY app helps people get their ECGs reviewed for abnormal rhythms\n",
      "within minutes. As head of ML at QALY:\n",
      "1. Trained ML models on ECG time-series data to automatically detect\n",
      "abnormal rhythms and annotate relevant beats.\n",
      "2. Deployed the ML models to production using cloud services. These models\n",
      "were deployed live and provided real-time insights within the QALY app.\n",
      "3. Built the iOS app that has been downloaded by more than 50,000 people on\n",
      "the App Store. \n",
      "Â  Page 1 of 3\n",
      "Â  Â \n",
      "4. Deployed multiple backend services using cloud tools.\n",
      "Schlumberger\n",
      "3 years 3 months\n",
      "Senior Data Scientist\n",
      "January 2020Â -Â October 2020Â  (10 months)\n",
      "San Francisco Bay Area\n",
      "1. Implemented deep generative modeling algorithms to model realistic\n",
      "geological properties conditioned on physical measurement. \n",
      "2. Worked on deep learning super-resolution algorithms to reconstruct high-\n",
      "resolution seismic data from low-resolution input. This helped reduce storage\n",
      "and communication costs associated with generating seismic visualizations. \n",
      "3. Worked on ML algorithms applied to reservoir simulations.\n",
      "Data Scientist\n",
      "August 2017Â -Â January 2020Â  (2 years 6 months)\n",
      "San Francisco Bay Area\n",
      "1. Applied 3D fully convolutional deep learning algorithms to segment relevant\n",
      "geological structures from seismic data \n",
      "2. Implemented natural language processing algorithms to extract relevant\n",
      "information from unstructured text reports.\n",
      "3. Worked on a new semantic inpainting algorithm using Pixel Constrained\n",
      "CNNs.\n",
      "Stanford University\n",
      "Graduate Research Assistant\n",
      "September 2016Â -Â June 2017Â  (10 months)\n",
      "United States\n",
      "I was a graduate research assistant in the Mobilize lab (http://\n",
      "mobilize.stanford.edu/) under Prof. Scott Delp. I worked on quantifying\n",
      "radiographic knee osteoarthritis severity using deep learning models.\n",
      "Schlumberger\n",
      "Data Science Intern\n",
      "June 2016Â -Â September 2016Â  (4 months)\n",
      "Worked on time-series alignment and deep learning algorithms to solve\n",
      "problems in automation. \n",
      "Indian Institute of Science\n",
      "Research Intern\n",
      "May 2013Â -Â July 2013Â  (3 months)\n",
      "Â  Page 2 of 3\n",
      "Â  Â \n",
      "Evaluated performance of Eigen value solvers for Parabolized Stability\n",
      "Equation (PSE)\n",
      "involving sparse matrices and optimized existing FORTRAN code-base to\n",
      "reduce\n",
      "execution time. Modeled nonparallel and nonlinear effects observed in the\n",
      "aeroacoustics of a jet engine.\n",
      "Education\n",
      "Stanford University\n",
      "Master of Science (M.S.),Â Computational and Applied\n",
      "Mathematics Â Â·Â (2015Â -Â 2017)\n",
      "Indian Institute of Technology, Madras\n",
      "M.Tech and B.Tech (Honours) in Aerospace EngineeringÂ  Â Â·Â (2010Â -Â 2015)\n",
      "Â  Page 3 of 3\n"
     ]
    }
   ],
   "source": [
    "suhas_linkedin_text = doc_reader.read_document(\"documents/suhas_suresha_linkedin_profile.pdf\")\n",
    "print (suhas_linkedin_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "429b066d-7257-48c7-b096-492975776b0e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in Suhas's LinkedIn profile = 617\n"
     ]
    }
   ],
   "source": [
    "def count_words(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Count the number of words in a string.\n",
    "    \"\"\"\n",
    "    # Remove extra whitespace and split into words\n",
    "    words = text.strip().split()\n",
    "    return len(words)\n",
    "\n",
    "print (f\"Number of words in Suhas's LinkedIn profile = {count_words(suhas_linkedin_text)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2c90aab-dc17-4a95-82bb-a2830195b991",
   "metadata": {},
   "source": [
    "#### Let's now ask questions based on this PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "595251a4-353b-4a79-a00a-5dacc9b49b6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: Where was Suhas born?\n",
      "A: I cannot find the answer in the provided context.\n",
      "\n",
      "Q: Where did he complete his masters from?\n",
      "A: Stanford University.\n",
      "\n",
      "Q: Where does Suhas work now?\n",
      "A: Suhas currently works at Adobe as a Senior Machine Learning Engineer.\n",
      "\n",
      "Q: How many countries has Suhas visited?\n",
      "A: I cannot find the answer in the provided context.\n",
      "\n",
      "Q: How many languages does Suhas know?\n",
      "A: Suhas knows four languages: English, Kannada, Hindi, and Tamil.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "Use the provided context to answer the question. \n",
    "If you cannot find the answer from the provided context, say \"I cannot find the answer in the provided context.\"\n",
    "\"\"\"\n",
    "\n",
    "document_path = \"documents/suhas_suresha_linkedin_profile.pdf\"\n",
    "\n",
    "questions = [\n",
    "    \"Where was Suhas born?\", ## not in the pdf\n",
    "    \"Where did he complete his masters from?\",\n",
    "    \"Where does Suhas work now?\",  \n",
    "    \"How many countries has Suhas visited?\", # Not in the pdf\n",
    "    \"How many languages does Suhas know?\" \n",
    "]\n",
    "\n",
    "for question in questions:\n",
    "    print(f\"\\nQ: {question}\")\n",
    "    print(f\"A: {ask_llm_from_document(document_path, question, system_instruction)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b7e212-994f-42c0-b099-66b8e594283a",
   "metadata": {},
   "source": [
    "### Okay that worked quite well! ðŸŽ‰\n",
    "\n",
    "### Let's try something more ambitious now. We'll feed the entire text of \"The Great Gatsby\" novel, which has 51,257 words, into our agent.  \n",
    "\n",
    "This file is located at `documents/the_great_gatsby.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "56a65547-b826-44b0-9565-3433354e24c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in The Great Gatsby novel = 51257\n"
     ]
    }
   ],
   "source": [
    "gatsby_text = doc_reader.read_document(\"documents/the_great_gatsby.txt\")\n",
    "print (f\"Number of words in The Great Gatsby novel = {count_words(gatsby_text)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6b3b0c28-fc7f-477d-a1d2-44223166742f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "BadRequestError",
     "evalue": "Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 69572 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadRequestError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m document_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdocuments/the_great_gatsby.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      8\u001b[0m question \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWhat does Gatsby\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms real name turn out to be, and why did he change it?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 9\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[43mask_llm_from_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mQ: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00manswer\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[25], line 6\u001b[0m, in \u001b[0;36mask_llm_from_document\u001b[0;34m(file_path, question, system_instruction, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ask a question using text content from a document file as context.\"\"\"\u001b[39;00m\n\u001b[1;32m      5\u001b[0m document_text_content \u001b[38;5;241m=\u001b[39m doc_reader\u001b[38;5;241m.\u001b[39mread_document(file_path)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mask_llm_with_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocument_text_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msystem_instruction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m, in \u001b[0;36mask_llm_with_context\u001b[0;34m(context, question, system_instruction, model)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Ask a question with specific context\"\"\"\u001b[39;00m\n\u001b[1;32m      6\u001b[0m question_with_context \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124m    Context: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcontext\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124m    Question: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquestion\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;124m    \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mask\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser_question\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion_with_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m    \u001b[49m\u001b[43msystem_message\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msystem_instruction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "File \u001b[0;32m~/Documents/personal_code/pctopus/pctopus/llm/openai_llm.py:37\u001b[0m, in \u001b[0;36mOpenAILLM.ask\u001b[0;34m(self, user_question, system_message, model)\u001b[0m\n\u001b[1;32m     34\u001b[0m     messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: system_message})\n\u001b[1;32m     35\u001b[0m messages\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: user_question})\n\u001b[0;32m---> 37\u001b[0m response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m response\u001b[38;5;241m.\u001b[39mchoices[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mmessage\u001b[38;5;241m.\u001b[39mcontent\n",
      "File \u001b[0;32m~/miniconda3/envs/pctopus/lib/python3.11/site-packages/openai/_utils/_utils.py:275\u001b[0m, in \u001b[0;36mrequired_args.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    273\u001b[0m             msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing required argument: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mquote(missing[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(msg)\n\u001b[0;32m--> 275\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pctopus/lib/python3.11/site-packages/openai/resources/chat/completions.py:829\u001b[0m, in \u001b[0;36mCompletions.create\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    788\u001b[0m \u001b[38;5;129m@required_args\u001b[39m([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m], [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    789\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate\u001b[39m(\n\u001b[1;32m    790\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    826\u001b[0m     timeout: \u001b[38;5;28mfloat\u001b[39m \u001b[38;5;241m|\u001b[39m httpx\u001b[38;5;241m.\u001b[39mTimeout \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m|\u001b[39m NotGiven \u001b[38;5;241m=\u001b[39m NOT_GIVEN,\n\u001b[1;32m    827\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ChatCompletion \u001b[38;5;241m|\u001b[39m Stream[ChatCompletionChunk]:\n\u001b[1;32m    828\u001b[0m     validate_response_format(response_format)\n\u001b[0;32m--> 829\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_post\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/chat/completions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbody\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmaybe_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m            \u001b[49m\u001b[43m{\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmessages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodel\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43maudio\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43maudio\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfrequency_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrequency_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunction_call\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunction_call\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfunctions\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunctions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    839\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogit_bias\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogit_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    840\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlogprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    841\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_completion_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_completion_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    842\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax_tokens\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    843\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    844\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmodalities\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodalities\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    845\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mn\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    846\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mparallel_tool_calls\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparallel_tool_calls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    847\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mprediction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    848\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpresence_penalty\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mpresence_penalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    849\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mresponse_format\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mresponse_format\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    850\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mseed\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    851\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mservice_tier\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mservice_tier\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    852\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstop\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    853\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    854\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    855\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstream_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    856\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtemperature\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    857\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtool_choice\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtool_choice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    858\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtools\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtools\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_logprobs\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_logprobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    860\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtop_p\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    861\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43muser\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    862\u001b[0m \u001b[43m            \u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    863\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcompletion_create_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCompletionCreateParams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    864\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    865\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmake_request_options\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    866\u001b[0m \u001b[43m            \u001b[49m\u001b[43mextra_headers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_headers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_query\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_query\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mextra_body\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextra_body\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\n\u001b[1;32m    867\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    868\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mChatCompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    869\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    870\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mStream\u001b[49m\u001b[43m[\u001b[49m\u001b[43mChatCompletionChunk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    871\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pctopus/lib/python3.11/site-packages/openai/_base_client.py:1280\u001b[0m, in \u001b[0;36mSyncAPIClient.post\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpost\u001b[39m(\n\u001b[1;32m   1267\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   1268\u001b[0m     path: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1275\u001b[0m     stream_cls: \u001b[38;5;28mtype\u001b[39m[_StreamT] \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1276\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m ResponseT \u001b[38;5;241m|\u001b[39m _StreamT:\n\u001b[1;32m   1277\u001b[0m     opts \u001b[38;5;241m=\u001b[39m FinalRequestOptions\u001b[38;5;241m.\u001b[39mconstruct(\n\u001b[1;32m   1278\u001b[0m         method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpost\u001b[39m\u001b[38;5;124m\"\u001b[39m, url\u001b[38;5;241m=\u001b[39mpath, json_data\u001b[38;5;241m=\u001b[39mbody, files\u001b[38;5;241m=\u001b[39mto_httpx_files(files), \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39moptions\n\u001b[1;32m   1279\u001b[0m     )\n\u001b[0;32m-> 1280\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m cast(ResponseT, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m~/miniconda3/envs/pctopus/lib/python3.11/site-packages/openai/_base_client.py:957\u001b[0m, in \u001b[0;36mSyncAPIClient.request\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    955\u001b[0m     retries_taken \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m--> 957\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    958\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast_to\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcast_to\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    959\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    961\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstream_cls\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    962\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretries_taken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mretries_taken\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    963\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/pctopus/lib/python3.11/site-packages/openai/_base_client.py:1061\u001b[0m, in \u001b[0;36mSyncAPIClient._request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1058\u001b[0m         err\u001b[38;5;241m.\u001b[39mresponse\u001b[38;5;241m.\u001b[39mread()\n\u001b[1;32m   1060\u001b[0m     log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRe-raising status error\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1061\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_status_error_from_response(err\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1063\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_response(\n\u001b[1;32m   1064\u001b[0m     cast_to\u001b[38;5;241m=\u001b[39mcast_to,\n\u001b[1;32m   1065\u001b[0m     options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1069\u001b[0m     retries_taken\u001b[38;5;241m=\u001b[39mretries_taken,\n\u001b[1;32m   1070\u001b[0m )\n",
      "\u001b[0;31mBadRequestError\u001b[0m: Error code: 400 - {'error': {'message': \"This model's maximum context length is 16385 tokens. However, your messages resulted in 69572 tokens. Please reduce the length of the messages.\", 'type': 'invalid_request_error', 'param': 'messages', 'code': 'context_length_exceeded'}}"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "Use the provided context to answer the question. \n",
    "If you cannot find the answer from the provided context, say \"I cannot find the answer in the provided context.\"\n",
    "\"\"\"\n",
    "\n",
    "document_path = \"documents/the_great_gatsby.txt\"\n",
    "\n",
    "question = \"What does Gatsby's real name turn out to be, and why did he change it?\"\n",
    "answer = ask_llm_from_document(document_path, question, system_instruction)\n",
    "\n",
    "print(f\"\\nQ: {question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d17872d-cc32-4f62-b36e-20580bae8578",
   "metadata": {},
   "source": [
    "### Uh-oh! Seems like our agent has hit a wall! ðŸ¤–ðŸ’¥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d2d2b75-3881-46cb-abf4-707672ec8d45",
   "metadata": {},
   "source": [
    "If you look at the error message, it says `This model's maximum context length is 16385 tokens. However, your messages resulted in 69576 tokens. Please reduce the length of the messages.` Let's dive deeper into why this happened. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea5b7fa-781c-45ae-9422-257206a9d101",
   "metadata": {},
   "source": [
    "## The Great Token Mystery: Understanding Tokens and Context Windows ðŸ”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f423f368-6077-4ae4-8abf-9955b6d2d2f5",
   "metadata": {},
   "source": [
    "### What are Tokens? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e427cd0-b5ce-4f61-a467-b9f14780ce06",
   "metadata": {},
   "source": [
    "If you remember, we breifly introduced **tokens** in Chapter 1 as the **building blocks of language that the LLM processes**. Let's revisit this concept now. \n",
    "\n",
    "Think of tokens as the LLM's **\"thought chunks\"** - they're the bite-sized pieces it uses to process text. Just like how we humans break down sentences into words, LLMs break down text into tokens. However, tokens don't map one-to-one with words - they can be:\n",
    "\n",
    "- Common single words (\"the\", \"and\")\n",
    "- Parts of words (\"ing\", \"ed\", \"pre\")\n",
    "- Characters (especially for uncommon words)\n",
    "- Special characters and spaces\n",
    "\n",
    "This flexible tokenization helps the model be more efficient. For example, common words like \"the\", \"and\" or \"beautiful\" might be single tokens because they appear so frequently. Meanwhile, rare words like \"discombobulated\" might be split into several tokens (\"dis\", \"comb\", \"ob\", \"ulated\").\n",
    "\n",
    "For example, let's see how a simple sentence gets broken down into tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0855d395-7112-44f4-bc01-c233a4bb1a3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Text: Hello, world!\n",
      "Token count: 4\n",
      "Tokens: ['Hello', ',', ' world', '!']\n",
      "\n",
      "==================================================\n",
      "Text: thank you very much\n",
      "Token count: 4\n",
      "Tokens: ['thank', ' you', ' very', ' much']\n",
      "\n",
      "==================================================\n",
      "Text: discombobulated\n",
      "Token count: 4\n",
      "Tokens: ['dis', 'comb', 'ob', 'ulated']\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "\n",
    "def show_tokenization(text: str, model: str = \"gpt-3.5-turbo\") -> None:\n",
    "    \"\"\"Show how text gets split into tokens\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    tokens = encoding.encode(text)\n",
    "    \n",
    "    # Decode each token individually to see the splits\n",
    "    token_texts = [encoding.decode_single_token_bytes(token).decode('utf-8', errors='replace') for token in tokens]\n",
    "    \n",
    "    print(f\"Text: {text}\")\n",
    "    print(f\"Token count: {len(tokens)}\")\n",
    "    print(\"Tokens:\", token_texts)\n",
    "\n",
    "# Let's try some examples\n",
    "examples = [\n",
    "    \"Hello, world!\", # Common phrase\n",
    "    \"thank you very much\",  # Common phrase\n",
    "    \"discombobulated\",  # Uncommon word\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    show_tokenization(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25455551-5799-4b0d-b505-cb1afc7d1510",
   "metadata": {},
   "source": [
    "### Different Models, Different Tokenizers ðŸ”„"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b65db30-5c1d-4517-80ad-9aa298894b15",
   "metadata": {},
   "source": [
    "Each LLM family typically comes with its own tokenizer - the tool that breaks text into tokens. This means the same text might be split very differently depending on which model you're using:\n",
    "\n",
    "\n",
    "#### GPT-2 Tokenizer (\"gpt2\")\n",
    "- Basic tokenization approach\n",
    "- Vocabulary size: ~50k tokens\n",
    "- Primarily focused on English text\n",
    "\n",
    "#### GPT-3.5/GPT-4 Tokenizer (\"cl100k_base\")\n",
    "- Optimized for modern internet text\n",
    "- Strong handling of code and technical content\n",
    "- Vocabulary size: ~100k tokens\n",
    "- Improved efficiency in token usage\n",
    "\n",
    "#### GPT-4o Optimized Tokenizer (\"o200k_base\")\n",
    "- Significantly larger vocabulary (~200k tokens)\n",
    "- Enhanced multilingual support\n",
    "- More efficient tokenization for non-English words\n",
    "- Particularly improved performance for:\n",
    "  - Russian\n",
    "  - Korean\n",
    "  - Vietnamese\n",
    "  - Chinese\n",
    "  - Japanese\n",
    "  - Turkish\n",
    "\n",
    "These differences matter because:\n",
    "\n",
    "1. The same text will use different numbers of tokens in different models\n",
    "2. Cost calculations vary (since you pay per token)\n",
    "3. Context window limits affect different amounts of text\n",
    "4. Some models handle certain content types better\n",
    "\n",
    "Let's see these differences in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1fa307d4-f17b-4f1a-8f6d-56b4043a2d8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Comparing different tokenizers:\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Test case: Common Phrase\n",
      "\n",
      "GPT-2 tokenization of 'thank you':\n",
      "Token count: 2\n",
      "Tokens: ['thank', ' you']\n",
      "\n",
      "GPT-3.5-turbo tokenization of 'thank you':\n",
      "Token count: 2\n",
      "Tokens: ['thank', ' you']\n",
      "\n",
      "Token count comparison:\n",
      "               Token Count\n",
      "GPT-2                    2\n",
      "GPT-3.5-turbo            2\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Test case: Non-English\n",
      "\n",
      "GPT-2 tokenization of 'ã“ã‚“ã«ã¡ã¯':\n",
      "Token count: 6\n",
      "Tokens: ['ã“', 'ã‚“', 'ã«', 'ï¿½', 'ï¿½', 'ã¯']\n",
      "\n",
      "GPT-3.5-turbo tokenization of 'ã“ã‚“ã«ã¡ã¯':\n",
      "Token count: 1\n",
      "Tokens: ['ã“ã‚“ã«ã¡ã¯']\n",
      "\n",
      "Token count comparison:\n",
      "               Token Count\n",
      "GPT-2                    6\n",
      "GPT-3.5-turbo            1\n",
      "==================================================\n",
      "\n",
      "ðŸ“ Test case: Code\n",
      "\n",
      "GPT-2 tokenization of '\n",
      "    for i in range(1, 6):\n",
      "        for j in range(1, 6):\n",
      "            result = i * j\n",
      "            print(f\"{result:3}\", end=\"\")\n",
      "        print()  # This creates a new line after each row\n",
      "    ':\n",
      "Token count: 97\n",
      "Tokens: ['\\n', ' ', ' ', ' ', ' for', ' i', ' in', ' range', '(', '1', ',', ' 6', '):', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' for', ' j', ' in', ' range', '(', '1', ',', ' 6', '):', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' result', ' =', ' i', ' *', ' j', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' print', '(', 'f', '\"', '{', 'result', ':', '3', '}', '\",', ' end', '=\"', '\")', '\\n', ' ', ' ', ' ', ' ', ' ', ' ', ' ', ' print', '()', ' ', ' #', ' This', ' creates', ' a', ' new', ' line', ' after', ' each', ' row', '\\n', ' ', ' ', ' ', ' ']\n",
      "\n",
      "GPT-3.5-turbo tokenization of '\n",
      "    for i in range(1, 6):\n",
      "        for j in range(1, 6):\n",
      "            result = i * j\n",
      "            print(f\"{result:3}\", end=\"\")\n",
      "        print()  # This creates a new line after each row\n",
      "    ':\n",
      "Token count: 55\n",
      "Tokens: ['\\n', '   ', ' for', ' i', ' in', ' range', '(', '1', ',', ' ', '6', '):\\n', '       ', ' for', ' j', ' in', ' range', '(', '1', ',', ' ', '6', '):\\n', '           ', ' result', ' =', ' i', ' *', ' j', '\\n', '           ', ' print', '(f', '\"{', 'result', ':', '3', '}\",', ' end', '=\"\")\\n', '       ', ' print', '()', ' ', ' #', ' This', ' creates', ' a', ' new', ' line', ' after', ' each', ' row', '\\n', '    ']\n",
      "\n",
      "Token count comparison:\n",
      "               Token Count\n",
      "GPT-2                   97\n",
      "GPT-3.5-turbo           55\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "import pandas as pd\n",
    "from typing import Dict\n",
    "\n",
    "def compare_tokenizers(text: str) -> Dict[str, int]:\n",
    "    \"\"\"Compare how different tokenizers process the same text.\"\"\"\n",
    "    # Define models with different tokenizers\n",
    "    models = {\n",
    "        \"GPT-2\": \"gpt2\",\n",
    "        \"GPT-3.5-turbo\": \"gpt-3.5-turbo\", \n",
    "    }\n",
    "    \n",
    "    # Count tokens for each model\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        encoding = tiktoken.encoding_for_model(model)\n",
    "        tokens = encoding.encode(text)\n",
    "        results[name] = len(tokens)\n",
    "        \n",
    "        # Show the actual tokens for analysis\n",
    "        token_texts = [encoding.decode_single_token_bytes(token).decode('utf-8', errors='replace') for token in tokens]\n",
    "        print(f\"\\n{name} tokenization of '{text}':\")\n",
    "        print(f\"Token count: {len(tokens)}\")\n",
    "        print(\"Tokens:\", token_texts)\n",
    "        \n",
    "    return results\n",
    "\n",
    "# Test cases demonstrating key differences\n",
    "test_cases = {\n",
    "    \"Common Phrase\": \"thank you\",\n",
    "    \"Non-English\": \"ã“ã‚“ã«ã¡ã¯\",  # Japanese\n",
    "    \"Code\": \"\"\"\n",
    "    for i in range(1, 6):\n",
    "        for j in range(1, 6):\n",
    "            result = i * j\n",
    "            print(f\"{result:3}\", end=\"\")\n",
    "        print()  # This creates a new line after each row\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "print(\"ðŸ” Comparing different tokenizers:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Run comparison for each test case\n",
    "for case_name, text in test_cases.items():\n",
    "    print(f\"\\nðŸ“ Test case: {case_name}\")\n",
    "    results = compare_tokenizers(text)\n",
    "    \n",
    "    # Show token count differences\n",
    "    df = pd.DataFrame([results]).T\n",
    "    df.columns = ['Token Count']\n",
    "    print(\"\\nToken count comparison:\")\n",
    "    print(df)\n",
    "    print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aae095c2-c29e-48f4-9011-2ef84497bcbb",
   "metadata": {},
   "source": [
    "#### We see that the tokenizer for `GPT-3.5-turbo` model reduces the number of tokens needed for non-english text and python code when copmapred to `GPT-2`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed903c08-a196-45ab-ab61-8e91f7e4989f",
   "metadata": {},
   "source": [
    "### Context Windows: The LLM's Working Memory ðŸªŸ\n",
    "\n",
    "The context window is like the LLM's working memory - it's the maximum amount of tokens that the model can \"think about\" at once. This includes:\n",
    "\n",
    "- The system message (instructions we give)\n",
    "- The context (like our document text)\n",
    "- The user's question\n",
    "- The LLM's response\n",
    "\n",
    "Different models have different context window sizes:\n",
    "\n",
    "| Model | Context Window | \n",
    "|:--|:--|\n",
    "| GPT-3.5-Turbo | 16,385 tokens |\n",
    "| GPT-4o-mini | 128,000 tokens | \n",
    "| GPT-4o | 128,000 tokens |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b923d99-b79a-4ac2-a723-c12bb40bfdfb",
   "metadata": {},
   "source": [
    "## Understanding Why The Great Gatsby Example Failed ðŸ”"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361a781a-c5db-4346-acc2-af12d1f9e819",
   "metadata": {},
   "source": [
    "Remember how our attempt to analyze The Great Gatsby failed with an error about exceeding the context window? Now that we understand tokens and context windows, let's break down exactly what happened.\n",
    "\n",
    "Let's calculate the tokens we were trying to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "262df80b-a432-40b5-bb43-ba5deb01cb2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens in the Great Gatsby text: 69,499 tokens\n"
     ]
    }
   ],
   "source": [
    "def count_tokens(text: str, model: str = \"gpt-3.5-turbo\") -> int:\n",
    "    \"\"\"Count tokens for a given text using specified model's tokenizer\"\"\"\n",
    "    encoding = tiktoken.encoding_for_model(model)\n",
    "    return len(encoding.encode(text))\n",
    "\n",
    "gatsby_text = doc_reader.read_document(\"documents/the_great_gatsby.txt\")\n",
    "gatsby_tokens = count_tokens(gatsby_text)\n",
    "\n",
    "print(f\"Tokens in the Great Gatsby text: {gatsby_tokens:,} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc10e2fb-f0c0-477a-9f5c-87df9b374020",
   "metadata": {},
   "source": [
    "#### Remember, `GPT-3.5-turbo` has a context window of **16,385 tokens**. Just The Great Gatsby alone uses about **69,499 tokens!** That's not even including the tokens needed for system instructions and the user question. That's why we got the error message. It's like trying to fit an entire novel into a small notepad."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8bd2ef-2c63-4d78-8189-443e327fe38a",
   "metadata": {},
   "source": [
    "### Let's Fix It: Using GPT-4o-mini ðŸ› ï¸\n",
    "\n",
    "GPT-4o-mini has a much larger context window of **128,000 tokens**. Let's modify our code to use it instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6d61841b-0108-4f83-aca1-30030daee791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Q: What does Gatsby's real name turn out to be, and why did he change it?\n",
      "A: Gatsby's real name is James Gatz. He changed it to Jay Gatsby when he was seventeen as part of his reinvention of himself. He adopted the new name after he met Dan Cody, a wealthy copper magnate, who became a mentor to him. Gatsby wanted to leave behind his poor background and aspiring to rise in society, he created a new identity that matched his ambitions and dreams of success.\n"
     ]
    }
   ],
   "source": [
    "system_instruction = \"\"\"\n",
    "Use the provided context to answer the question. \n",
    "If you cannot find the answer from the provided context, say \"I cannot find the answer in the provided context.\"\n",
    "\"\"\"\n",
    "\n",
    "document_path = \"documents/the_great_gatsby.txt\"\n",
    "\n",
    "question = \"What does Gatsby's real name turn out to be, and why did he change it?\"\n",
    "answer = ask_llm_from_document(document_path, question, system_instruction, model='gpt-4o-mini')\n",
    "\n",
    "print(f\"\\nQ: {question}\")\n",
    "print(f\"A: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3755034-accb-4c83-a40e-30fec57e64f9",
   "metadata": {},
   "source": [
    "## The Cost of Large Contexts: Measuring Latency ðŸ“Š"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0587a4-cfbb-4326-8b5a-2b22d1f39c7d",
   "metadata": {},
   "source": [
    "You might have noticed that running the previous code might have taken some time before you got a response back. That's because typically, **the more context we feed into an LLM, the longer it takes to process.** \n",
    "\n",
    "Let's measure the response time of an LLM for different text length to prove this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbd917-5d65-418f-b5c3-dcb23f762361",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tiktoken\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def generate_text_with_token_count(target_token_count: int) -> str:\n",
    "    \"\"\"Generate text with approximately the specified number of tokens\"\"\"\n",
    "    tokenizer = tiktoken.encoding_for_model(\"gpt-4o-mini\")\n",
    "    base_text = \"This is a sample text for testing language model response times.\"\n",
    "    base_tokens = len(tokenizer.encode(base_text))\n",
    "    \n",
    "    repetitions = target_token_count // base_tokens\n",
    "    context = base_text * repetitions\n",
    "    \n",
    "    current_tokens = len(tokenizer.encode(context))\n",
    "    while current_tokens < target_token_count:\n",
    "        context += \"Additional text. \"\n",
    "        current_tokens = len(tokenizer.encode(context))\n",
    "    \n",
    "    return context\n",
    "\n",
    "def measure_response_time(token_count: int, model: str) -> float:\n",
    "    \"\"\"Measure response time for different token counts\"\"\"\n",
    "    context = generate_text_with_token_count(token_count)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    response = ask_llm_with_context(\n",
    "        system_instruction=\"Just return the word ok, and nothing else.\",\n",
    "        context=context,\n",
    "        question=\"\",\n",
    "        model=model\n",
    "    )\n",
    "    end_time = time.time()\n",
    "    \n",
    "    return end_time - start_time\n",
    "\n",
    "# Test different token counts with multiple runs\n",
    "token_counts = [50000, 25000, 5000]\n",
    "num_runs = 10\n",
    "all_response_times = []  # Store all runs for each token count\n",
    "\n",
    "for count in token_counts:\n",
    "    run_times = []\n",
    "    for _ in range(num_runs):\n",
    "        time_4 = measure_response_time(count, \"gpt-4o-mini\")\n",
    "        run_times.append(time_4)\n",
    "    all_response_times.append(run_times)\n",
    "\n",
    "# Calculate medians and p95\n",
    "median_response_times = [np.median(times) for times in all_response_times]\n",
    "p95_response_times = [np.percentile(times, 95) for times in all_response_times]\n",
    "\n",
    "# Create visualization\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "# Plot individual runs\n",
    "for i, times in enumerate(all_response_times):\n",
    "    plt.scatter([token_counts[i]] * num_runs, times, \n",
    "                alpha=0.5, color='lightblue', \n",
    "                label='Individual runs' if i == 0 else \"\")\n",
    "\n",
    "# Plot medians with a line\n",
    "plt.plot(token_counts, median_response_times, \n",
    "         marker='o', color='darkblue', linewidth=2, \n",
    "         label='Median response time')\n",
    "\n",
    "# Add p95 \"error\" bars (showing range up to p95)\n",
    "for i, (count, median, p95) in enumerate(zip(token_counts, median_response_times, p95_response_times)):\n",
    "    plt.vlines(count, median, p95, color='darkblue', linestyle='--', alpha=0.5)\n",
    "    plt.scatter([count], [p95], color='darkblue', alpha=0.5, marker='_', s=100)\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel('Token Count')\n",
    "plt.ylabel('Response Time (seconds)')\n",
    "plt.title('Impact of Token Count on Response Time\\n(10 runs per token count)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Reverse x-axis to show larger token counts on the left\n",
    "plt.gca().invert_xaxis()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print statistical summary\n",
    "print(\"\\nStatistical Summary:\")\n",
    "for i, count in enumerate(token_counts):\n",
    "    times = all_response_times[i]\n",
    "    print(f\"\\nToken Count: {count}\")\n",
    "    print(f\"Median Response Time: {np.median(times):.2f} seconds\")\n",
    "    print(f\"95th Percentile: {np.percentile(times, 95):.2f} seconds\")\n",
    "    print(f\"Min Time: {np.min(times):.2f} seconds\")\n",
    "    print(f\"Max Time: {np.max(times):.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b660d7-cba8-4a88-bdd5-bd9254ff92e7",
   "metadata": {},
   "source": [
    "#### As you can see from the graph, there's a clear correlation between context size and response time. Typically, the larger the context, the longer we have to wait for an answer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6ce5de5-0530-4b9c-91d5-258f84256a67",
   "metadata": {},
   "source": [
    "## Problems with Our Current Approach ðŸ¤”\n",
    "\n",
    "Our current method of feeding entire documents into the context window has several significant drawbacks:\n",
    "\n",
    "Context Window Limits\n",
    "\n",
    "Large documents exceed model context windows\n",
    "Multiple documents compound the problem\n",
    "Important information might get cut off\n",
    "\n",
    "\n",
    "Cost Implications\n",
    "\n",
    "Paying for tokens we might not need\n",
    "Each request includes full document cost\n",
    "Inefficient for repeated queries\n",
    "\n",
    "\n",
    "Latency Issues\n",
    "\n",
    "Larger contexts = slower responses\n",
    "Poor user experience with long wait times\n",
    "Resource intensive\n",
    "\n",
    "\n",
    "Scalability Problems\n",
    "\n",
    "Cannot handle growing document collections\n",
    "Limited by single context window\n",
    "No way to process large document libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605802ce-9b3e-412a-a497-fd23aff46274",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
